{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNQmi+3UH07C2QPj1NWAf1g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9883b5ae16bb40c5912dc5ed9a9842ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7896468873c4f45b6f724c47dd054db",
              "IPY_MODEL_e6ee146bcfc84affaa2707730ce1d86f",
              "IPY_MODEL_2acac7026e7943fba3590f0de3822ba7"
            ],
            "layout": "IPY_MODEL_4ce056770d6946738ad348265c7cf8b0"
          }
        },
        "d7896468873c4f45b6f724c47dd054db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87b9404cb54c4422b6d1c3d49821d0ce",
            "placeholder": "​",
            "style": "IPY_MODEL_23c885f298e54c8193195761a1a02c19",
            "value": "Tokenizing train set (num_proc=4): 100%"
          }
        },
        "e6ee146bcfc84affaa2707730ce1d86f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40db46c9f0674bc4b4f85dfa97585003",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a713f4d70d1d46dc9b713281b1c49c66",
            "value": 1000
          }
        },
        "2acac7026e7943fba3590f0de3822ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e2c50e95c094af0b94f9b15a228f905",
            "placeholder": "​",
            "style": "IPY_MODEL_1838cd1cd4ec47eeb236ebda13a28092",
            "value": " 1000/1000 [00:01&lt;00:00, 1102.22 examples/s]"
          }
        },
        "4ce056770d6946738ad348265c7cf8b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b9404cb54c4422b6d1c3d49821d0ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c885f298e54c8193195761a1a02c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40db46c9f0674bc4b4f85dfa97585003": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a713f4d70d1d46dc9b713281b1c49c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e2c50e95c094af0b94f9b15a228f905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1838cd1cd4ec47eeb236ebda13a28092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caef8b1ffe414883bf3797bc42ab228a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc06939e7f114e24a114186f1e7603b5",
              "IPY_MODEL_56c0983eec174161aa878d960bbd5d41",
              "IPY_MODEL_c5aad42a2bf948bf8236ba8d65678500"
            ],
            "layout": "IPY_MODEL_ca25e80bd6bb4cdcb1cc3b21a4cf937d"
          }
        },
        "bc06939e7f114e24a114186f1e7603b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64c6161f17ef4ba2b0b9667f27b96829",
            "placeholder": "​",
            "style": "IPY_MODEL_6ac63b5dfb824850a2939083a686cc12",
            "value": "Tokenizing validation set (num_proc=4): 100%"
          }
        },
        "56c0983eec174161aa878d960bbd5d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6452c60779774036a2342d2f7d572b4b",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8da4175855174d3992ae0ad19c3dc65b",
            "value": 1000
          }
        },
        "c5aad42a2bf948bf8236ba8d65678500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b8b895f89f54d9aa69215fe6f2c47a9",
            "placeholder": "​",
            "style": "IPY_MODEL_0652793573644a8a9ba88320664edc53",
            "value": " 1000/1000 [00:01&lt;00:00, 914.93 examples/s]"
          }
        },
        "ca25e80bd6bb4cdcb1cc3b21a4cf937d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64c6161f17ef4ba2b0b9667f27b96829": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ac63b5dfb824850a2939083a686cc12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6452c60779774036a2342d2f7d572b4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8da4175855174d3992ae0ad19c3dc65b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b8b895f89f54d9aa69215fe6f2c47a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0652793573644a8a9ba88320664edc53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f39dc3e8ec749dfa67847f04934d2f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c5285189fa0416c9529175eabe49210",
              "IPY_MODEL_38e71c957ee04635a16846ec8ac58139",
              "IPY_MODEL_85ccffa38ea74e739f75a405ae728f64"
            ],
            "layout": "IPY_MODEL_bd0921df48b347ffa89b36cd19266474"
          }
        },
        "0c5285189fa0416c9529175eabe49210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15d7e60347114b088bd831bb1c27ec1d",
            "placeholder": "​",
            "style": "IPY_MODEL_bdf9bbca54774cb489b299b982c972b1",
            "value": "Tokenizing test set (num_proc=4): 100%"
          }
        },
        "38e71c957ee04635a16846ec8ac58139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd10a545c53343ab9e2e9bc2a8e93b71",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c007fbc1856f40f9ab81d50eb7cbf74d",
            "value": 1000
          }
        },
        "85ccffa38ea74e739f75a405ae728f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40c177777a1f4117a6d83001590c8ebf",
            "placeholder": "​",
            "style": "IPY_MODEL_800856983b6f4cea9771499715ad5090",
            "value": " 1000/1000 [00:01&lt;00:00, 1267.47 examples/s]"
          }
        },
        "bd0921df48b347ffa89b36cd19266474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d7e60347114b088bd831bb1c27ec1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdf9bbca54774cb489b299b982c972b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd10a545c53343ab9e2e9bc2a8e93b71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c007fbc1856f40f9ab81d50eb7cbf74d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40c177777a1f4117a6d83001590c8ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "800856983b6f4cea9771499715ad5090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsl5710/greenland/blob/main/GREENLAND_Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Setup & Installation"
      ],
      "metadata": {
        "id": "W3Bum5rWN9oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and upgrade necessary libraries\n",
        "!pip install --quiet --upgrade pip\n",
        "!pip install --quiet --upgrade transformers\n",
        "!pip install --quiet --upgrade datasets\n",
        "!pip install --quiet --upgrade wandb\n",
        "!pip install --quiet git+https://github.com/huggingface/peft.git peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnCpombcOC4Z",
        "outputId": "e3e96f55-e18d-4112-97d8-d87b045fc568",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Import Libraries"
      ],
      "metadata": {
        "id": "52iDZjrEyY3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "import pandas as pd\n",
        "from typing import Dict, Optional\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PrefixTuningConfig,\n",
        "    PromptTuningConfig,\n",
        "    AdaLoraConfig,\n",
        "    IA3Config,\n",
        "    get_peft_model,\n",
        "    PeftModel,\n",
        "    TaskType,\n",
        "    PeftConfig\n",
        ")\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from peft import get_peft_model, LoraConfig, TaskType, AutoPeftModelForSequenceClassification\n",
        "from google.colab import drive\n",
        "from requests.exceptions import HTTPError"
      ],
      "metadata": {
        "id": "X0pZE3KRbLLw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Define Model Checkpoints"
      ],
      "metadata": {
        "id": "FbzH1OPlz13a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoints = {\n",
        "    \"MBERT_uncased\": {\n",
        "        \"path\": \"google-bert/bert-base-multilingual-uncased\",\n",
        "        \"max_length\": 512\n",
        "    },\n",
        "    # \"XLM_100\": {\n",
        "    #     \"path\": \"FacebookAI/xlm-mlm-100-1280\",\n",
        "    #     \"max_length\": 512\n",
        "    # },\n",
        "    # \"XLM_17\": {\n",
        "    #     \"path\": \"FacebookAI/xlm-mlm-17-1280\",\n",
        "    #     \"max_length\": 512\n",
        "    # },\n",
        "    # \"XLM-RoBERTa_xxl\": {\n",
        "    #     \"path\": \"facebook/xlm-roberta-xxl\",\n",
        "    #     \"max_length\": 512\n",
        "    # },\n",
        "    # \"mDeBERTa_v3_base\": {\n",
        "    #     \"path\": \"microsoft/mdeberta-v3-base\",\n",
        "    #     \"max_length\": 512\n",
        "    # },\n",
        "    # \"S-BERT_LaBSE\": {\n",
        "    #     \"path\": \"sentence-transformers/LaBSE\",\n",
        "    #     \"max_length\": 512\n",
        "    # },\n",
        "    # \"S-BERT_distiluse\": {\n",
        "    #     \"path\": \"sentence-transformers/distiluse-base-multilingual-cased\",\n",
        "    #     \"max_length\": 512\n",
        "    # },\n",
        "    # \"XLM-R_bernice\": {\n",
        "    #     \"path\": \"jhu-clsp/bernice\",\n",
        "    #     \"max_length\": 512\n",
        "    # },\n",
        "    # \"XLM-T_twitter\": {\n",
        "    #     \"path\": \"cardiffnlp/twitter-xlm-roberta-base\",\n",
        "    #     \"max_length\": 512\n",
        "    # },\n",
        "    # \"XLM-E_align\": {\n",
        "    #     \"path\": \"microsoft/xlm-align-base\",\n",
        "    #     \"max_length\": 512\n",
        "    # },\n",
        "    # \"XLM-E_infoxlm_large\": {\n",
        "    #     \"path\": \"microsoft/infoxlm-large\",\n",
        "    #     \"max_length\": 512\n",
        "    # },\n",
        "    # \"XLM-V_base\": {\n",
        "    #     \"path\": \"facebook/xlm-v-base\",\n",
        "    #     \"max_length\": 512\n",
        "    # }\n",
        "}\n",
        "\n",
        "\n",
        "# model_checkpoints = {\n",
        "#     \"MBERT_uncased\": \"google-bert/bert-base-multilingual-uncased\",\n",
        "#     # \"MBERT_cased\": \"google-bert/bert-base-multilingual-cased\",\n",
        "#     \"XLM_100\": \"FacebookAI/xlm-mlm-100-1280\",\n",
        "#     \"XLM_17\": \"FacebookAI/xlm-mlm-17-1280\",\n",
        "#     # \"XLM-RoBERTa_large\": \"FacebookAI/xlm-roberta-large\",\n",
        "#     # \"XLM-RoBERTa_base\": \"FacebookAI/xlm-roberta-base\",\n",
        "#     # \"XLM-RoBERTa_xl\": \"facebook/xlm-roberta-xl\",\n",
        "#     \"XLM-RoBERTa_xxl\": \"facebook/xlm-roberta-xxl\",\n",
        "#     \"mDeBERTa_v3_base\": \"microsoft/mdeberta-v3-base\",\n",
        "#     # \"M-distilBERT\": \"distilbert/distilbert-base-multilingual-cased\",\n",
        "#     \"S-BERT_LaBSE\": \"sentence-transformers/LaBSE\",\n",
        "#     \"S-BERT_distiluse\": \"sentence-transformers/distiluse-base-multilingual-cased\",\n",
        "#     \"XLM-R_bernice\": \"jhu-clsp/bernice\",\n",
        "#     \"XLM-T_twitter\": \"cardiffnlp/twitter-xlm-roberta-base\",\n",
        "#     \"XLM-E_align\": \"microsoft/xlm-align-base\",\n",
        "#     # \"XLM-E_infoxlm_base\": \"microsoft/infoxlm-base\",\n",
        "#     \"XLM-E_infoxlm_large\": \"microsoft/infoxlm-large\",\n",
        "#     \"XLM-V_base\": \"facebook/xlm-v-base\"\n",
        "# }\n",
        "\n",
        "# model_checkpoints = {\n",
        "    # \"MBERT_uncased\": \"google-bert/bert-base-multilingual-uncased\",\n",
        "    # \"XLM_100\": \"FacebookAI/xlm-mlm-100-1280\",\n",
        "    # \"XLM_17\": \"FacebookAI/xlm-mlm-17-1280\",\n",
        "    # \"XLM-RoBERTa_xxl\": \"facebook/xlm-roberta-xxl\",\n",
        "    # \"mDeBERTa_v3_base\": \"microsoft/mdeberta-v3-base\",\n",
        "    # \"S-BERT_LaBSE\": \"sentence-transformers/LaBSE\",\n",
        "    # \"S-BERT_distiluse\": \"sentence-transformers/distiluse-base-multilingual-cased\",\n",
        "    # \"XLM-R_bernice\": \"jhu-clsp/bernice\",\n",
        "    # \"XLM-T_twitter\": \"cardiffnlp/twitter-xlm-roberta-base\",\n",
        "    # \"XLM-E_align\": \"microsoft/xlm-align-base\",\n",
        "    #     \"XLM-E_infoxlm_large\": \"microsoft/infoxlm-large\",\n",
        "    # \"XLM-V_base\": \"facebook/xlm-v-base\"\n",
        "# }\n",
        "\n"
      ],
      "metadata": {
        "id": "DBdcWP1pz6CA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Authenticate and Initialize"
      ],
      "metadata": {
        "id": "CciQJFQ3y5cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "!huggingface-cli login --token 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\n",
        "\n",
        "# Authenticate with W&B\n",
        "wandb.login(key=\"1b5caf38a8b6ada0e6918798e9379b2ea764062d\")\n",
        "wandb.init(project=\"greenland\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "tYgicB7QzBcj",
        "outputId": "382fd334-c02c-4dae-b12e-bddece966b88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `greenland` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `greenland`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjasonsamlucas\u001b[0m (\u001b[33mpike\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241108_053032-ve6n8na7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/pike/greenland/runs/ve6n8na7' target=\"_blank\">snowy-wave-250</a></strong> to <a href='https://wandb.ai/pike/greenland' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/pike/greenland' target=\"_blank\">https://wandb.ai/pike/greenland</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/pike/greenland/runs/ve6n8na7' target=\"_blank\">https://wandb.ai/pike/greenland/runs/ve6n8na7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/pike/greenland/runs/ve6n8na7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7d790f2026b0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Define Save Paths and Ensure Directories Exist"
      ],
      "metadata": {
        "id": "fmhXLfnvzME0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define save locations\n",
        "local_save_path = \"/content/sample_data/best_models/\"\n",
        "drive_save_path = \"/content/drive/MyDrive/GREENLAND/Modeling/Best_models/\"\n",
        "results_dir = \"/content/drive/MyDrive/GREENLAND/Results/\"\n",
        "\n",
        "# Ensure save directories exist\n",
        "os.makedirs(local_save_path, exist_ok=True)\n",
        "os.makedirs(drive_save_path, exist_ok=True)\n",
        "os.makedirs(results_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "fsuy1-W4zVCf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Load and Process the Dataset"
      ],
      "metadata": {
        "id": "N_cUoRK-zXCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load datasets from CSV files in Google Drive\n",
        "# train_df = pd.read_csv('/content/drive/MyDrive/GREENLAND/Datasets/Consolidated_Data/Experiment_Training_Splits/train_data.csv')\n",
        "# val_df = pd.read_csv('/content/drive/MyDrive/GREENLAND/Datasets/Consolidated_Data/Experiment_Training_Splits/val_data.csv')\n",
        "# test_df = pd.read_csv('/content/drive/MyDrive/GREENLAND/Datasets/Consolidated_Data/Experiment_Training_Splits/test_data.csv')\n",
        "\n",
        "# # Convert to Hugging Face Dataset format\n",
        "# train_dataset = Dataset.from_pandas(train_df)\n",
        "# val_dataset = Dataset.from_pandas(val_df)\n",
        "# test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "\n",
        "# # Combine datasets into a dictionary for easy access\n",
        "# dataset = {\n",
        "#     \"train\": train_dataset,\n",
        "#     \"validation\": val_dataset,\n",
        "#     \"test\": test_dataset\n",
        "# }\n",
        "\n",
        "\n",
        "# Load datasets from CSV files in Google Drive\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/GREENLAND/Datasets/Consolidated_Data/Experiment_Training_Splits/train_data.csv')\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/GREENLAND/Datasets/Consolidated_Data/Experiment_Training_Splits/val_data.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/GREENLAND/Datasets/Consolidated_Data/Experiment_Training_Splits/test_data.csv')\n",
        "\n",
        "# Sample 1000 examples from each dataset with random seed for reproducibility\n",
        "train_df_sampled = train_df.sample(n=1000, random_state=42)\n",
        "val_df_sampled = val_df.sample(n=1000, random_state=42)\n",
        "test_df_sampled = test_df.sample(n=1000, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df_sampled)\n",
        "val_dataset = Dataset.from_pandas(val_df_sampled)\n",
        "test_dataset = Dataset.from_pandas(test_df_sampled)\n",
        "\n",
        "# Combine datasets into a dictionary for easy access\n",
        "dataset = {\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": val_dataset,\n",
        "    \"test\": test_dataset\n",
        "}\n",
        "\n",
        "print(\"Dataset sizes after sampling:\")\n",
        "print(f\"Train: {len(train_dataset)}\")\n",
        "print(f\"Validation: {len(val_dataset)}\")\n",
        "print(f\"Test: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "YM4oJ57pzdri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae5bd62-d8d8-4763-df2b-e7b70f4ca3fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset sizes after sampling:\n",
            "Train: 1000\n",
            "Validation: 1000\n",
            "Test: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Define Dataset Processing Functions"
      ],
      "metadata": {
        "id": "GuZkjbIp1AJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_dataset(dataset):\n",
        "    print(\"\\nDataset Verification:\")\n",
        "    for split in dataset.keys():\n",
        "        print(f\"\\n{split.capitalize()} set:\")\n",
        "        print(\"Number of examples:\", len(dataset[split]))\n",
        "        print(\"Features:\", dataset[split].features)\n",
        "        print(\"Sample labels:\", dataset[split][\"label\"][:5])\n",
        "        # Check data type differently\n",
        "        print(\"Label type:\", type(dataset[split][\"label\"]))\n",
        "        # Print first few label types to understand the structure\n",
        "        print(\"Sample label types:\", [type(label) for label in dataset[split][\"label\"][:5]])\n",
        "    return True\n",
        "\n",
        "def tokenize_datasets(model_name, dataset):\n",
        "    model_info = model_checkpoints[model_name]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_info[\"path\"])\n",
        "    max_length = model_info[\"max_length\"]\n",
        "\n",
        "    print(f\"Using max_length={max_length} for model {model_name}\")\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        # Convert boolean labels to integers, handling list input\n",
        "        labels = [int(label) if isinstance(label, bool) else int(bool(label))\n",
        "                 for label in examples[\"label\"]]\n",
        "\n",
        "        # Tokenize the text\n",
        "        tokenized = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors=None\n",
        "        )\n",
        "\n",
        "        # Add converted labels to the tokenized output\n",
        "        tokenized[\"labels\"] = labels\n",
        "        return tokenized\n",
        "\n",
        "    # Print sample of data before tokenization\n",
        "    print(\"\\nBefore tokenization:\")\n",
        "    print(\"Sample of original labels:\", dataset[\"train\"][\"label\"][:5])\n",
        "    print(\"Original label type:\", type(dataset[\"train\"][\"label\"][0]))\n",
        "\n",
        "    tokenized_data = {\n",
        "        split: data.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            batch_size=1000,\n",
        "            num_proc=4,\n",
        "            remove_columns=data.column_names,\n",
        "            desc=f\"Tokenizing {split} set\"\n",
        "        )\n",
        "        for split, data in dataset.items()\n",
        "    }\n",
        "\n",
        "    # Verify the processed labels\n",
        "    print(\"\\nAfter tokenization:\")\n",
        "    print(\"Sample of processed labels:\", tokenized_data[\"train\"][\"labels\"][:5])\n",
        "    print(\"Processed label type:\", type(tokenized_data[\"train\"][\"labels\"][0]))\n",
        "\n",
        "    return tokenized_data\n",
        "\n",
        "def analyze_text_lengths(dataset):\n",
        "    \"\"\"\n",
        "    Analyze text lengths in the dataset without tokenization first\n",
        "    \"\"\"\n",
        "    # Get raw text lengths\n",
        "    lengths = [len(text.split()) for text in dataset[\"train\"][\"text\"]]\n",
        "\n",
        "    stats = {\n",
        "        \"average_length\": sum(lengths)/len(lengths),\n",
        "        \"max_length\": max(lengths),\n",
        "        \"median_length\": sorted(lengths)[len(lengths)//2],\n",
        "        \"95th_percentile\": sorted(lengths)[int(len(lengths)*0.95)],\n",
        "        \"length_distribution\": {\n",
        "            \"< 128 words\": sum(1 for l in lengths if l < 128),\n",
        "            \"128-256 words\": sum(1 for l in lengths if 128 <= l < 256),\n",
        "            \"256-512 words\": sum(1 for l in lengths if 256 <= l < 512),\n",
        "            \"> 512 words\": sum(1 for l in lengths if l >= 512)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Calculate percentages for distribution\n",
        "    total_samples = len(lengths)\n",
        "    stats[\"length_distribution_percent\"] = {\n",
        "        k: (v/total_samples * 100) for k, v in stats[\"length_distribution\"].items()\n",
        "    }\n",
        "\n",
        "    print(\"\\nText Length Analysis (word-based):\")\n",
        "    print(f\"Average length: {stats['average_length']:.1f} words\")\n",
        "    print(f\"Median length: {stats['median_length']} words\")\n",
        "    print(f\"Max length: {stats['max_length']} words\")\n",
        "    print(f\"95th percentile: {stats['95th_percentile']} words\")\n",
        "    print(\"\\nLength Distribution:\")\n",
        "    for category, count in stats[\"length_distribution\"].items():\n",
        "        percentage = stats[\"length_distribution_percent\"][category]\n",
        "        print(f\"{category}: {count} texts ({percentage:.1f}%)\")\n",
        "\n",
        "    # Character-based analysis\n",
        "    char_lengths = [len(text) for text in dataset[\"train\"][\"text\"]]\n",
        "    stats[\"char_stats\"] = {\n",
        "        \"average_length\": sum(char_lengths)/len(char_lengths),\n",
        "        \"max_length\": max(char_lengths),\n",
        "        \"median_length\": sorted(char_lengths)[len(char_lengths)//2],\n",
        "        \"95th_percentile\": sorted(char_lengths)[int(len(char_lengths)*0.95)]\n",
        "    }\n",
        "\n",
        "    print(\"\\nCharacter-based Analysis:\")\n",
        "    print(f\"Average length: {stats['char_stats']['average_length']:.1f} characters\")\n",
        "    print(f\"Median length: {stats['char_stats']['median_length']} characters\")\n",
        "    print(f\"Max length: {stats['char_stats']['max_length']} characters\")\n",
        "    print(f\"95th percentile: {stats['char_stats']['95th_percentile']} characters\")\n",
        "\n",
        "    return stats\n",
        "\n"
      ],
      "metadata": {
        "id": "b9J4Pt8T1RKy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Define Loss Functions"
      ],
      "metadata": {
        "id": "2vpUzaJHz-W2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedFocalLoss(torch.nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2):\n",
        "        super(WeightedFocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        # Apply softmax for multi-class probabilities\n",
        "        probs = torch.softmax(logits, dim=1)[:, 1]  # Probability for positive class\n",
        "        labels = labels.float()\n",
        "        BCE_loss = torch.nn.functional.binary_cross_entropy(probs, labels, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "        return F_loss.mean()\n",
        "\n",
        "class SymmetricCrossEntropyLoss(torch.nn.Module):\n",
        "    def __init__(self, alpha=0.1, beta=1.0):\n",
        "        super(SymmetricCrossEntropyLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        ce_loss = torch.nn.functional.cross_entropy(logits, labels)\n",
        "        labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=logits.size(-1))\n",
        "        rce_loss = -((torch.softmax(logits, dim=1) * labels_one_hot).sum(dim=-1).log().mean())\n",
        "        return self.alpha * ce_loss + self.beta * rce_loss\n",
        "\n",
        "\n",
        "class ModifiedBCEWithLogitsLoss(torch.nn.Module):\n",
        "    def forward(self, logits, labels):\n",
        "        # Ensure logits are the right shape (batch_size, num_classes)\n",
        "        if len(logits.shape) == 1:\n",
        "            logits = logits.unsqueeze(-1)\n",
        "        # Get the positive class logits\n",
        "        pos_logits = logits[:, 1]\n",
        "        return torch.nn.functional.binary_cross_entropy_with_logits(\n",
        "            pos_logits, labels.float(), reduction='mean'\n",
        "        )\n",
        "\n",
        "class ModifiedSquaredBCEWithLogitsLoss(torch.nn.Module):\n",
        "    def forward(self, logits, labels):\n",
        "        # Convert labels to float and ensure correct shape\n",
        "        labels = labels.float().view(-1)\n",
        "\n",
        "        # Ensure logits are the right shape for binary classification\n",
        "        if len(logits.shape) > 1 and logits.shape[1] == 2:\n",
        "            logits = logits[:, 1]  # Take the logit for positive class\n",
        "\n",
        "        # Apply sigmoid to get probabilities\n",
        "        probs = torch.sigmoid(logits)\n",
        "        return torch.mean((probs - labels) ** 2)\n",
        "\n",
        "class ModifiedWeightedBinaryCrossEntropy(torch.nn.Module):\n",
        "    def __init__(self, pos_weight):\n",
        "        super().__init__()\n",
        "        self.pos_weight = pos_weight\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        # Convert labels to float and ensure correct shape\n",
        "        labels = labels.float().view(-1)\n",
        "\n",
        "        # Ensure logits are the right shape for binary classification\n",
        "        if len(logits.shape) > 1 and logits.shape[1] == 2:\n",
        "            logits = logits[:, 1]  # Take the logit for positive class\n",
        "\n",
        "        return torch.nn.functional.binary_cross_entropy_with_logits(\n",
        "            logits, labels, pos_weight=self.pos_weight, reduction='mean'\n",
        "        )\n",
        "\n",
        "class ModifiedSupervisedContrastiveCrossEntropyLoss(torch.nn.Module):\n",
        "    def __init__(self, temperature=0.07, lam=0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.lam = lam\n",
        "        self.ce_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        # Standard cross-entropy loss\n",
        "        ce_loss = self.ce_loss(logits, labels.long())\n",
        "\n",
        "        # Contrastive loss\n",
        "        normalized_logits = torch.nn.functional.normalize(logits, dim=1)\n",
        "        similarity_matrix = torch.matmul(normalized_logits, normalized_logits.t()) / self.temperature\n",
        "\n",
        "        # Create mask for positive pairs\n",
        "        labels = labels.view(-1, 1)\n",
        "        mask = (labels == labels.t()).float()\n",
        "\n",
        "        # Compute contrastive loss\n",
        "        exp_sim = torch.exp(similarity_matrix)\n",
        "        log_prob = similarity_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True))\n",
        "\n",
        "        # Compute mean of positive pairs\n",
        "        mask_sum = mask.sum(dim=1)\n",
        "        mask_sum = torch.clamp(mask_sum, min=1e-8)  # Avoid division by zero\n",
        "        con_loss = (mask * log_prob).sum(dim=1) / mask_sum\n",
        "        con_loss = -con_loss.mean()\n",
        "\n",
        "        # Combine losses\n",
        "        return self.lam * ce_loss + (1 - self.lam) * con_loss"
      ],
      "metadata": {
        "id": "Zo0XodDEd9MU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Loss Functions Factory"
      ],
      "metadata": {
        "id": "zkDSx51heQMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_functions(device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    return {\n",
        "        \"CrossEntropyLoss\": torch.nn.CrossEntropyLoss().to(device),\n",
        "        \"BCEWithLogitsLoss\": ModifiedBCEWithLogitsLoss().to(device),\n",
        "        \"SquaredBCEWithLogitsLoss\": ModifiedSquaredBCEWithLogitsLoss().to(device),\n",
        "        \"WeightedBinaryCrossEntropy\": ModifiedWeightedBinaryCrossEntropy(\n",
        "            pos_weight=torch.tensor([3.0]).to(device)\n",
        "        ).to(device),\n",
        "        \"WeightedFocalLoss\": WeightedFocalLoss(\n",
        "            alpha=0.25,\n",
        "            gamma=2\n",
        "        ).to(device),\n",
        "        \"SymmetricCrossEntropy\": SymmetricCrossEntropyLoss(\n",
        "            alpha=0.1,\n",
        "            beta=1.0\n",
        "        ).to(device),\n",
        "        \"SupervisedContrastiveCrossEntropyLoss\": ModifiedSupervisedContrastiveCrossEntropyLoss(\n",
        "            temperature=0.07,\n",
        "            lam=0.5\n",
        "        ).to(device)\n",
        "    }"
      ],
      "metadata": {
        "id": "4QYl8aOgeQv6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Evaluation Metrics"
      ],
      "metadata": {
        "id": "KNdav7JueeYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(labels, preds)\n",
        "    except ValueError:\n",
        "        roc_auc = 0  # Handle cases where there might be only one class\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, preds),\n",
        "        'f1': f1_score(labels, preds, average='binary'),\n",
        "        'precision': precision_score(labels, preds, average='binary'),\n",
        "        'recall': recall_score(labels, preds, average='binary'),\n",
        "        'roc_auc': roc_auc\n",
        "    }\n"
      ],
      "metadata": {
        "id": "apiVQJCLea9b"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 11: Custom Trainer"
      ],
      "metadata": {
        "id": "qG0QCJfpf1Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, *args, loss_func=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_func = loss_func\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        if \"labels\" in inputs:\n",
        "            # Ensure labels are on the correct device\n",
        "            if not isinstance(inputs[\"labels\"], torch.Tensor):\n",
        "                inputs[\"labels\"] = torch.tensor(inputs[\"labels\"], device=self.args.device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        if self.loss_func is not None:\n",
        "            logits = outputs.logits\n",
        "            labels = inputs[\"labels\"]\n",
        "\n",
        "            # Binary classification losses need special handling\n",
        "            if isinstance(self.loss_func, (ModifiedSquaredBCEWithLogitsLoss,\n",
        "                                         ModifiedWeightedBinaryCrossEntropy,\n",
        "                                         ModifiedBCEWithLogitsLoss)):\n",
        "                loss = self.loss_func(logits, labels)\n",
        "            else:\n",
        "                # For cross entropy based losses\n",
        "                labels = labels.long()\n",
        "                loss = self.loss_func(logits, labels)\n",
        "        else:\n",
        "            loss = outputs.loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "7WLioVTp-P-8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 12: Model Save/Load Functions"
      ],
      "metadata": {
        "id": "oEGlYyH10F_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_with_fallback(trainer, model_name):\n",
        "    try:\n",
        "        trainer.push_to_hub(f\"jslai/{model_name}\")\n",
        "        print(f\"Model saved to Hugging Face Hub as jslai/{model_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save to Hugging Face Hub: {e}\")\n",
        "        try:\n",
        "            trainer.save_model(os.path.join(drive_save_path, model_name))\n",
        "            print(f\"Model saved to Google Drive at {drive_save_path}/{model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save to Google Drive: {e}\")\n",
        "            trainer.save_model(os.path.join(local_save_path, model_name))\n",
        "            print(f\"Model saved locally at {local_save_path}/{model_name}\")\n",
        "\n",
        "def load_best_model(model_name):\n",
        "    try:\n",
        "        print(f\"Attempting to load {model_name} from Hugging Face Hub.\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(f\"jslai/{model_name}\")\n",
        "    except (OSError, HTTPError) as e:\n",
        "        print(f\"Failed to load {model_name} from Hugging Face Hub: {e}\")\n",
        "        try:\n",
        "            google_drive_path = os.path.join(drive_save_path, model_name)\n",
        "            if os.path.isdir(google_drive_path):\n",
        "                print(f\"Attempting to load {model_name} from Google Drive.\")\n",
        "                model = AutoModelForSequenceClassification.from_pretrained(google_drive_path)\n",
        "            else:\n",
        "                raise OSError(f\"Directory {google_drive_path} does not exist on Google Drive.\")\n",
        "        except (OSError, HTTPError) as e:\n",
        "            print(f\"Failed to load {model_name} from Google Drive: {e}\")\n",
        "            try:\n",
        "                local_path = os.path.join(local_save_path, model_name)\n",
        "                if os.path.isdir(local_path):\n",
        "                    print(f\"Attempting to load {model_name} from local storage.\")\n",
        "                    model = AutoModelForSequenceClassification.from_pretrained(local_path)\n",
        "                else:\n",
        "                    raise OSError(f\"Directory {local_path} does not exist in local storage.\")\n",
        "            except (OSError, HTTPError) as e:\n",
        "                print(f\"Failed to load {model_name} from local storage: {e}\")\n",
        "                raise FileNotFoundError(f\"Model {model_name} could not be found in any location.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "BIpYCkc10HPB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 13: Training Functions"
      ],
      "metadata": {
        "id": "Cc1yaV6t0MVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def full_fine_tune_all_models(model_checkpoints, dataset, loss_functions=None):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if loss_functions is None:\n",
        "        loss_functions = get_loss_functions(device)\n",
        "\n",
        "    for model_name, model_info in model_checkpoints.items():\n",
        "        # Tokenize dataset for this model\n",
        "        try:\n",
        "            tokenized_data = tokenize_datasets(model_name, dataset)\n",
        "\n",
        "            for loss_fn_name, loss_fn in loss_functions.items():\n",
        "                print(f\"\\nTraining {model_name} with {loss_fn_name}\")\n",
        "                print(f\"Using device: {device}\")\n",
        "\n",
        "                try:\n",
        "                    # Clear CUDA cache\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                    # Initialize model\n",
        "                    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                        model_info[\"path\"],\n",
        "                        num_labels=2,\n",
        "                        problem_type=\"single_label_classification\"\n",
        "                    ).to(device)\n",
        "\n",
        "                    # Training arguments\n",
        "                    training_args = TrainingArguments(\n",
        "                        output_dir=f\"{local_save_path}/{model_name}_{loss_fn_name}_full_ft\",\n",
        "                        eval_strategy=\"epoch\",  # Updated from evaluation_strategy\n",
        "                        save_strategy=\"epoch\",\n",
        "                        learning_rate=2e-5,\n",
        "                        per_device_train_batch_size=8,\n",
        "                        per_device_eval_batch_size=8,\n",
        "                        num_train_epochs=3,\n",
        "                        weight_decay=0.01,\n",
        "                        load_best_model_at_end=True,\n",
        "                        metric_for_best_model=\"f1\",\n",
        "                        report_to=\"wandb\",\n",
        "                        logging_steps=100,\n",
        "                        fp16=True,\n",
        "                        fp16_backend=\"auto\",\n",
        "                        gradient_checkpointing=True,\n",
        "                        gradient_accumulation_steps=2,\n",
        "                        warmup_ratio=0.1,\n",
        "                        dataloader_num_workers=4,\n",
        "                        dataloader_pin_memory=True,\n",
        "                        seed=42,\n",
        "                        remove_unused_columns=False\n",
        "                    )\n",
        "\n",
        "                    # Initialize trainer\n",
        "                    trainer = CustomTrainer(\n",
        "                        model=model,\n",
        "                        args=training_args,\n",
        "                        train_dataset=tokenized_data[\"train\"],\n",
        "                        eval_dataset=tokenized_data[\"validation\"],\n",
        "                        compute_metrics=compute_metrics,\n",
        "                        loss_func=loss_fn\n",
        "                    )\n",
        "\n",
        "                    # Train and save\n",
        "                    trainer.train()\n",
        "                    # save_model_with_fallback(trainer, f\"{model_name}_{loss_fn_name}_full_ft\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error training {model_name} with {loss_fn_name}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "                finally:\n",
        "                    # Cleanup\n",
        "                    if 'trainer' in locals():\n",
        "                        del trainer\n",
        "                    if 'model' in locals():\n",
        "                        del model\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing model {model_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(\"\\nFull Fine-Tuning completed!\")\n",
        "    return\n",
        "\n",
        "def peft_fine_tune_all_models(model_checkpoints, dataset, loss_functions=None, peft_methods=None):\n",
        "    # Initialize device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Get loss functions if not provided\n",
        "    if loss_functions is None:\n",
        "        loss_functions = get_loss_functions(device)\n",
        "\n",
        "    def get_target_modules(model_path):\n",
        "        \"\"\"Get target modules based on model architecture\"\"\"\n",
        "        if \"bert\" in model_path.lower():\n",
        "            return [\n",
        "                f\"bert.encoder.layer.{i}.attention.self.query\" for i in range(12)\n",
        "            ] + [\n",
        "                f\"bert.encoder.layer.{i}.attention.self.key\" for i in range(12)\n",
        "            ] + [\n",
        "                f\"bert.encoder.layer.{i}.attention.self.value\" for i in range(12)\n",
        "            ] + [\n",
        "                f\"bert.encoder.layer.{i}.attention.output.dense\" for i in range(12)\n",
        "            ]\n",
        "        elif \"roberta\" in model_path.lower():\n",
        "            return [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"]\n",
        "        elif \"deberta\" in model_path.lower():\n",
        "            return [\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"]\n",
        "        else:\n",
        "            return [\"query\", \"key\", \"value\", \"dense\"]\n",
        "\n",
        "    def get_feedforward_modules(model_path):\n",
        "        \"\"\"Get feedforward modules based on model architecture\"\"\"\n",
        "        if \"bert\" in model_path.lower():\n",
        "            return [f\"bert.encoder.layer.{i}.intermediate.dense\" for i in range(12)] + \\\n",
        "                   [f\"bert.encoder.layer.{i}.output.dense\" for i in range(12)]\n",
        "        elif \"roberta\" in model_path.lower():\n",
        "            return [\"fc1\", \"fc2\"]\n",
        "        else:\n",
        "            return [\"dense\"]\n",
        "\n",
        "    # Define PEFT methods\n",
        "    if peft_methods is None:\n",
        "        peft_methods = {\n",
        "            \"lora\": lambda path, modules: LoraConfig(\n",
        "                task_type=TaskType.SEQ_CLS,\n",
        "                r=16,\n",
        "                lora_alpha=32,\n",
        "                lora_dropout=0.1,\n",
        "                bias=\"none\",\n",
        "                inference_mode=False,\n",
        "                target_modules=modules,\n",
        "                modules_to_save=[\"classifier\"]\n",
        "            ),\n",
        "            \"adalora\": lambda path, modules: AdaLoraConfig(\n",
        "                task_type=TaskType.SEQ_CLS,\n",
        "                init_r=12,\n",
        "                target_r=8,\n",
        "                beta1=0.85,\n",
        "                beta2=0.95,\n",
        "                tinit=200,\n",
        "                tfinal=1000,\n",
        "                deltaT=10,\n",
        "                lora_alpha=32,\n",
        "                target_modules=modules,\n",
        "                lora_dropout=0.1,\n",
        "                inference_mode=False\n",
        "            ),\n",
        "            \"prefix\": lambda path, _: PrefixTuningConfig(\n",
        "                task_type=TaskType.SEQ_CLS,\n",
        "                num_virtual_tokens=20,\n",
        "                prefix_projection=True,\n",
        "                encoder_hidden_size=768  # Will be updated based on model\n",
        "            ),\n",
        "            \"ia3\": lambda path, modules: IA3Config(\n",
        "                task_type=TaskType.SEQ_CLS,\n",
        "                target_modules=modules,\n",
        "                feedforward_modules=get_feedforward_modules(path),\n",
        "                inference_mode=False,\n",
        "                modules_to_not_convert=[\"classifier\"]\n",
        "            )\n",
        "        }\n",
        "\n",
        "    for model_name, model_info in model_checkpoints.items():\n",
        "        try:\n",
        "            # Get target modules for this model\n",
        "            target_modules = get_target_modules(model_info[\"path\"])\n",
        "\n",
        "            # Tokenize dataset specific to model\n",
        "            tokenized_data = tokenize_datasets(model_name, dataset)\n",
        "\n",
        "            for peft_name, peft_config_fn in peft_methods.items():\n",
        "                for loss_fn_name, loss_fn in loss_functions.items():\n",
        "                    try:\n",
        "                        print(f\"\\nFine-tuning {model_name} with PEFT ({peft_name}) using {loss_fn_name}\")\n",
        "                        print(f\"Using device: {device}\")\n",
        "\n",
        "                        # Initialize tokenizer and base model\n",
        "                        tokenizer = AutoTokenizer.from_pretrained(model_info[\"path\"])\n",
        "                        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                            model_info[\"path\"],\n",
        "                            num_labels=2,\n",
        "                            problem_type=\"single_label_classification\"\n",
        "                        )\n",
        "\n",
        "                        # Get PEFT configuration\n",
        "                        peft_config = peft_config_fn(model_info[\"path\"], target_modules)\n",
        "\n",
        "                        # Update encoder_hidden_size for prefix tuning\n",
        "                        if peft_name == \"prefix\":\n",
        "                            peft_config.encoder_hidden_size = base_model.config.hidden_size\n",
        "\n",
        "                        # Print available modules for debugging\n",
        "                        print(f\"\\nModel architecture for {model_name}:\")\n",
        "                        print(\"Available modules:\", [name for name, _ in base_model.named_modules()])\n",
        "\n",
        "                        # Get PEFT model\n",
        "                        model = get_peft_model(base_model, peft_config)\n",
        "                        print(f\"\\nTrainable parameters for {peft_name}:\")\n",
        "                        model.print_trainable_parameters()\n",
        "\n",
        "                        # Move model to device\n",
        "                        model = model.to(device)\n",
        "\n",
        "                        # Ensure loss function is on correct device\n",
        "                        loss_fn = loss_fn.to(device)\n",
        "\n",
        "                        # Define training arguments\n",
        "                        training_args = TrainingArguments(\n",
        "                            output_dir=f\"{local_save_path}/{model_name}_{loss_fn_name}_{peft_name}\",\n",
        "                            eval_strategy=\"epoch\",\n",
        "                            save_strategy=\"epoch\",\n",
        "                            learning_rate=2e-5,\n",
        "                            per_device_train_batch_size=8,\n",
        "                            per_device_eval_batch_size=8,\n",
        "                            num_train_epochs=3,\n",
        "                            weight_decay=0.01,\n",
        "                            load_best_model_at_end=True,\n",
        "                            metric_for_best_model=\"f1\",\n",
        "                            logging_dir=\"./logs\",\n",
        "                            report_to=\"wandb\",\n",
        "                            logging_steps=100,\n",
        "                            fp16=True,\n",
        "                            fp16_backend=\"auto\",\n",
        "                            gradient_checkpointing=True,\n",
        "                            gradient_accumulation_steps=4,\n",
        "                            optim=\"adamw_torch\",\n",
        "                            warmup_ratio=0.1,\n",
        "                            dataloader_num_workers=4,\n",
        "                            dataloader_pin_memory=True,\n",
        "                            seed=42\n",
        "                        )\n",
        "\n",
        "                        # Initialize trainer\n",
        "                        trainer = CustomTrainer(\n",
        "                            model=model,\n",
        "                            args=training_args,\n",
        "                            train_dataset=tokenized_data[\"train\"],\n",
        "                            eval_dataset=tokenized_data[\"validation\"],\n",
        "                            tokenizer=tokenizer,\n",
        "                            data_collator=DataCollatorWithPadding(tokenizer),\n",
        "                            compute_metrics=compute_metrics,\n",
        "                            loss_func=loss_fn\n",
        "                        )\n",
        "\n",
        "                        # Train the model\n",
        "                        trainer.train()\n",
        "\n",
        "                        # Save the model and adapter\n",
        "                        # output_dir = f\"{local_save_path}/{model_name}_{loss_fn_name}_{peft_name}\"\n",
        "                        # save_model_with_fallback(trainer, output_dir)\n",
        "                        # model.save_pretrained(f\"{output_dir}/adapter\")\n",
        "\n",
        "                        # Clear memory\n",
        "                        del model, base_model\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error training {model_name} with {peft_name} and {loss_fn_name}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing model {model_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(\"\\nPEFT fine-tuning completed!\")\n",
        "    return"
      ],
      "metadata": {
        "id": "qeOK06xo3zxk"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 14: Inference Function"
      ],
      "metadata": {
        "id": "qrJUwA9K3vct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference_and_save_results(model_checkpoints, test_df, results_dir):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    predictions_df_list = []\n",
        "\n",
        "    # Get list of all trained models\n",
        "    loss_functions = get_loss_functions(device)\n",
        "\n",
        "    for model_name, model_info in model_checkpoints.items():\n",
        "        # For each training method (full fine-tuning with different loss functions)\n",
        "        for loss_fn_name in loss_functions.keys():\n",
        "            try:\n",
        "                # Full fine-tuning model\n",
        "                full_ft_model_name = f\"{model_name}_{loss_fn_name}_full_ft\"\n",
        "                model = load_best_model(full_ft_model_name).to(device)\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_info[\"path\"])\n",
        "\n",
        "                inputs = tokenizer(\n",
        "                    list(test_df[\"text\"]),\n",
        "                    truncation=True,\n",
        "                    padding=True,\n",
        "                    max_length=model_info[\"max_length\"],\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**inputs)\n",
        "                    preds = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
        "\n",
        "                result_df = test_df.copy()\n",
        "                result_df[\"prediction\"] = preds\n",
        "                predictions_df_list.append((full_ft_model_name, result_df))\n",
        "\n",
        "                # PEFT model\n",
        "                peft_model_name = f\"{model_name}_{loss_fn_name}_peft_lora\"\n",
        "                peft_model_path = os.path.join(local_save_path, peft_model_name, \"adapter\")\n",
        "\n",
        "                if os.path.exists(peft_model_path):\n",
        "                    model = load_best_model(peft_model_name).to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model(**inputs)\n",
        "                        preds = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
        "\n",
        "                    result_df = test_df.copy()\n",
        "                    result_df[\"prediction\"] = preds\n",
        "                    predictions_df_list.append((peft_model_name, result_df))\n",
        "\n",
        "                # Clear memory\n",
        "                del model\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during inference for {model_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Save all predictions\n",
        "    for model_name, result_df in predictions_df_list:\n",
        "        result_file_path = os.path.join(results_dir, f\"{model_name}_predictions.csv\")\n",
        "        result_df.to_csv(result_file_path, index=False)\n",
        "        print(f\"Saved predictions for {model_name} to {result_file_path}\")\n"
      ],
      "metadata": {
        "id": "UJAA6rer39gE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 15: Main Execution"
      ],
      "metadata": {
        "id": "ArJQvEKVrPUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize device and wandb\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize loss functions\n",
        "    loss_functions = get_loss_functions(device)\n",
        "\n",
        "    # Analyze dataset once\n",
        "    print(\"Analyzing dataset text lengths...\")\n",
        "    dataset_stats = analyze_text_lengths(dataset)\n",
        "\n",
        "    # Run full fine-tuning\n",
        "    # print(\"\\nStarting Full Fine-Tuning with all models and loss functions...\")\n",
        "    # full_fine_tune_all_models(model_checkpoints, dataset, loss_functions)\n",
        "\n",
        "    # # Run PEFT fine-tuning\n",
        "    print(\"\\nStarting PEFT Fine-Tuning with LoRA on all models...\")\n",
        "    peft_fine_tune_all_models(model_checkpoints, dataset, loss_functions)\n",
        "\n",
        "    # # Run inference and save results\n",
        "    # print(\"\\nRunning inference and saving predictions...\")\n",
        "    # run_inference_and_save_results(model_checkpoints, test_df, results_dir)\n",
        "\n",
        "    print(\"\\nExperiments completed!\")\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9883b5ae16bb40c5912dc5ed9a9842ce",
            "d7896468873c4f45b6f724c47dd054db",
            "e6ee146bcfc84affaa2707730ce1d86f",
            "2acac7026e7943fba3590f0de3822ba7",
            "4ce056770d6946738ad348265c7cf8b0",
            "87b9404cb54c4422b6d1c3d49821d0ce",
            "23c885f298e54c8193195761a1a02c19",
            "40db46c9f0674bc4b4f85dfa97585003",
            "a713f4d70d1d46dc9b713281b1c49c66",
            "8e2c50e95c094af0b94f9b15a228f905",
            "1838cd1cd4ec47eeb236ebda13a28092",
            "caef8b1ffe414883bf3797bc42ab228a",
            "bc06939e7f114e24a114186f1e7603b5",
            "56c0983eec174161aa878d960bbd5d41",
            "c5aad42a2bf948bf8236ba8d65678500",
            "ca25e80bd6bb4cdcb1cc3b21a4cf937d",
            "64c6161f17ef4ba2b0b9667f27b96829",
            "6ac63b5dfb824850a2939083a686cc12",
            "6452c60779774036a2342d2f7d572b4b",
            "8da4175855174d3992ae0ad19c3dc65b",
            "8b8b895f89f54d9aa69215fe6f2c47a9",
            "0652793573644a8a9ba88320664edc53",
            "9f39dc3e8ec749dfa67847f04934d2f9",
            "0c5285189fa0416c9529175eabe49210",
            "38e71c957ee04635a16846ec8ac58139",
            "85ccffa38ea74e739f75a405ae728f64",
            "bd0921df48b347ffa89b36cd19266474",
            "15d7e60347114b088bd831bb1c27ec1d",
            "bdf9bbca54774cb489b299b982c972b1",
            "cd10a545c53343ab9e2e9bc2a8e93b71",
            "c007fbc1856f40f9ab81d50eb7cbf74d",
            "40c177777a1f4117a6d83001590c8ebf",
            "800856983b6f4cea9771499715ad5090"
          ]
        },
        "id": "StmhOYiTrQXR",
        "outputId": "adaaa078-9215-457c-87d0-c64e24cb3576"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Analyzing dataset text lengths...\n",
            "\n",
            "Text Length Analysis (word-based):\n",
            "Average length: 241.3 words\n",
            "Median length: 137 words\n",
            "Max length: 2611 words\n",
            "95th percentile: 833 words\n",
            "\n",
            "Length Distribution:\n",
            "< 128 words: 488 texts (48.8%)\n",
            "128-256 words: 170 texts (17.0%)\n",
            "256-512 words: 199 texts (19.9%)\n",
            "> 512 words: 143 texts (14.3%)\n",
            "\n",
            "Character-based Analysis:\n",
            "Average length: 1543.6 characters\n",
            "Median length: 925 characters\n",
            "Max length: 16013 characters\n",
            "95th percentile: 5156 characters\n",
            "\n",
            "Starting PEFT Fine-Tuning with LoRA on all models...\n",
            "Using max_length=512 for model MBERT_uncased\n",
            "\n",
            "Before tokenization:\n",
            "Sample of original labels: [True, True, True, True, True]\n",
            "Original label type: <class 'bool'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train set (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9883b5ae16bb40c5912dc5ed9a9842ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing validation set (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "caef8b1ffe414883bf3797bc42ab228a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing test set (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f39dc3e8ec749dfa67847f04934d2f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After tokenization:\n",
            "Sample of processed labels: [1, 1, 1, 1, 1]\n",
            "Processed label type: <class 'int'>\n",
            "\n",
            "Fine-tuning MBERT_uncased with PEFT (lora) using CrossEntropyLoss\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model architecture for MBERT_uncased:\n",
            "Available modules: ['', 'bert', 'bert.embeddings', 'bert.embeddings.word_embeddings', 'bert.embeddings.position_embeddings', 'bert.embeddings.token_type_embeddings', 'bert.embeddings.LayerNorm', 'bert.embeddings.dropout', 'bert.encoder', 'bert.encoder.layer', 'bert.encoder.layer.0', 'bert.encoder.layer.0.attention', 'bert.encoder.layer.0.attention.self', 'bert.encoder.layer.0.attention.self.query', 'bert.encoder.layer.0.attention.self.key', 'bert.encoder.layer.0.attention.self.value', 'bert.encoder.layer.0.attention.self.dropout', 'bert.encoder.layer.0.attention.output', 'bert.encoder.layer.0.attention.output.dense', 'bert.encoder.layer.0.attention.output.LayerNorm', 'bert.encoder.layer.0.attention.output.dropout', 'bert.encoder.layer.0.intermediate', 'bert.encoder.layer.0.intermediate.dense', 'bert.encoder.layer.0.intermediate.intermediate_act_fn', 'bert.encoder.layer.0.output', 'bert.encoder.layer.0.output.dense', 'bert.encoder.layer.0.output.LayerNorm', 'bert.encoder.layer.0.output.dropout', 'bert.encoder.layer.1', 'bert.encoder.layer.1.attention', 'bert.encoder.layer.1.attention.self', 'bert.encoder.layer.1.attention.self.query', 'bert.encoder.layer.1.attention.self.key', 'bert.encoder.layer.1.attention.self.value', 'bert.encoder.layer.1.attention.self.dropout', 'bert.encoder.layer.1.attention.output', 'bert.encoder.layer.1.attention.output.dense', 'bert.encoder.layer.1.attention.output.LayerNorm', 'bert.encoder.layer.1.attention.output.dropout', 'bert.encoder.layer.1.intermediate', 'bert.encoder.layer.1.intermediate.dense', 'bert.encoder.layer.1.intermediate.intermediate_act_fn', 'bert.encoder.layer.1.output', 'bert.encoder.layer.1.output.dense', 'bert.encoder.layer.1.output.LayerNorm', 'bert.encoder.layer.1.output.dropout', 'bert.encoder.layer.2', 'bert.encoder.layer.2.attention', 'bert.encoder.layer.2.attention.self', 'bert.encoder.layer.2.attention.self.query', 'bert.encoder.layer.2.attention.self.key', 'bert.encoder.layer.2.attention.self.value', 'bert.encoder.layer.2.attention.self.dropout', 'bert.encoder.layer.2.attention.output', 'bert.encoder.layer.2.attention.output.dense', 'bert.encoder.layer.2.attention.output.LayerNorm', 'bert.encoder.layer.2.attention.output.dropout', 'bert.encoder.layer.2.intermediate', 'bert.encoder.layer.2.intermediate.dense', 'bert.encoder.layer.2.intermediate.intermediate_act_fn', 'bert.encoder.layer.2.output', 'bert.encoder.layer.2.output.dense', 'bert.encoder.layer.2.output.LayerNorm', 'bert.encoder.layer.2.output.dropout', 'bert.encoder.layer.3', 'bert.encoder.layer.3.attention', 'bert.encoder.layer.3.attention.self', 'bert.encoder.layer.3.attention.self.query', 'bert.encoder.layer.3.attention.self.key', 'bert.encoder.layer.3.attention.self.value', 'bert.encoder.layer.3.attention.self.dropout', 'bert.encoder.layer.3.attention.output', 'bert.encoder.layer.3.attention.output.dense', 'bert.encoder.layer.3.attention.output.LayerNorm', 'bert.encoder.layer.3.attention.output.dropout', 'bert.encoder.layer.3.intermediate', 'bert.encoder.layer.3.intermediate.dense', 'bert.encoder.layer.3.intermediate.intermediate_act_fn', 'bert.encoder.layer.3.output', 'bert.encoder.layer.3.output.dense', 'bert.encoder.layer.3.output.LayerNorm', 'bert.encoder.layer.3.output.dropout', 'bert.encoder.layer.4', 'bert.encoder.layer.4.attention', 'bert.encoder.layer.4.attention.self', 'bert.encoder.layer.4.attention.self.query', 'bert.encoder.layer.4.attention.self.key', 'bert.encoder.layer.4.attention.self.value', 'bert.encoder.layer.4.attention.self.dropout', 'bert.encoder.layer.4.attention.output', 'bert.encoder.layer.4.attention.output.dense', 'bert.encoder.layer.4.attention.output.LayerNorm', 'bert.encoder.layer.4.attention.output.dropout', 'bert.encoder.layer.4.intermediate', 'bert.encoder.layer.4.intermediate.dense', 'bert.encoder.layer.4.intermediate.intermediate_act_fn', 'bert.encoder.layer.4.output', 'bert.encoder.layer.4.output.dense', 'bert.encoder.layer.4.output.LayerNorm', 'bert.encoder.layer.4.output.dropout', 'bert.encoder.layer.5', 'bert.encoder.layer.5.attention', 'bert.encoder.layer.5.attention.self', 'bert.encoder.layer.5.attention.self.query', 'bert.encoder.layer.5.attention.self.key', 'bert.encoder.layer.5.attention.self.value', 'bert.encoder.layer.5.attention.self.dropout', 'bert.encoder.layer.5.attention.output', 'bert.encoder.layer.5.attention.output.dense', 'bert.encoder.layer.5.attention.output.LayerNorm', 'bert.encoder.layer.5.attention.output.dropout', 'bert.encoder.layer.5.intermediate', 'bert.encoder.layer.5.intermediate.dense', 'bert.encoder.layer.5.intermediate.intermediate_act_fn', 'bert.encoder.layer.5.output', 'bert.encoder.layer.5.output.dense', 'bert.encoder.layer.5.output.LayerNorm', 'bert.encoder.layer.5.output.dropout', 'bert.encoder.layer.6', 'bert.encoder.layer.6.attention', 'bert.encoder.layer.6.attention.self', 'bert.encoder.layer.6.attention.self.query', 'bert.encoder.layer.6.attention.self.key', 'bert.encoder.layer.6.attention.self.value', 'bert.encoder.layer.6.attention.self.dropout', 'bert.encoder.layer.6.attention.output', 'bert.encoder.layer.6.attention.output.dense', 'bert.encoder.layer.6.attention.output.LayerNorm', 'bert.encoder.layer.6.attention.output.dropout', 'bert.encoder.layer.6.intermediate', 'bert.encoder.layer.6.intermediate.dense', 'bert.encoder.layer.6.intermediate.intermediate_act_fn', 'bert.encoder.layer.6.output', 'bert.encoder.layer.6.output.dense', 'bert.encoder.layer.6.output.LayerNorm', 'bert.encoder.layer.6.output.dropout', 'bert.encoder.layer.7', 'bert.encoder.layer.7.attention', 'bert.encoder.layer.7.attention.self', 'bert.encoder.layer.7.attention.self.query', 'bert.encoder.layer.7.attention.self.key', 'bert.encoder.layer.7.attention.self.value', 'bert.encoder.layer.7.attention.self.dropout', 'bert.encoder.layer.7.attention.output', 'bert.encoder.layer.7.attention.output.dense', 'bert.encoder.layer.7.attention.output.LayerNorm', 'bert.encoder.layer.7.attention.output.dropout', 'bert.encoder.layer.7.intermediate', 'bert.encoder.layer.7.intermediate.dense', 'bert.encoder.layer.7.intermediate.intermediate_act_fn', 'bert.encoder.layer.7.output', 'bert.encoder.layer.7.output.dense', 'bert.encoder.layer.7.output.LayerNorm', 'bert.encoder.layer.7.output.dropout', 'bert.encoder.layer.8', 'bert.encoder.layer.8.attention', 'bert.encoder.layer.8.attention.self', 'bert.encoder.layer.8.attention.self.query', 'bert.encoder.layer.8.attention.self.key', 'bert.encoder.layer.8.attention.self.value', 'bert.encoder.layer.8.attention.self.dropout', 'bert.encoder.layer.8.attention.output', 'bert.encoder.layer.8.attention.output.dense', 'bert.encoder.layer.8.attention.output.LayerNorm', 'bert.encoder.layer.8.attention.output.dropout', 'bert.encoder.layer.8.intermediate', 'bert.encoder.layer.8.intermediate.dense', 'bert.encoder.layer.8.intermediate.intermediate_act_fn', 'bert.encoder.layer.8.output', 'bert.encoder.layer.8.output.dense', 'bert.encoder.layer.8.output.LayerNorm', 'bert.encoder.layer.8.output.dropout', 'bert.encoder.layer.9', 'bert.encoder.layer.9.attention', 'bert.encoder.layer.9.attention.self', 'bert.encoder.layer.9.attention.self.query', 'bert.encoder.layer.9.attention.self.key', 'bert.encoder.layer.9.attention.self.value', 'bert.encoder.layer.9.attention.self.dropout', 'bert.encoder.layer.9.attention.output', 'bert.encoder.layer.9.attention.output.dense', 'bert.encoder.layer.9.attention.output.LayerNorm', 'bert.encoder.layer.9.attention.output.dropout', 'bert.encoder.layer.9.intermediate', 'bert.encoder.layer.9.intermediate.dense', 'bert.encoder.layer.9.intermediate.intermediate_act_fn', 'bert.encoder.layer.9.output', 'bert.encoder.layer.9.output.dense', 'bert.encoder.layer.9.output.LayerNorm', 'bert.encoder.layer.9.output.dropout', 'bert.encoder.layer.10', 'bert.encoder.layer.10.attention', 'bert.encoder.layer.10.attention.self', 'bert.encoder.layer.10.attention.self.query', 'bert.encoder.layer.10.attention.self.key', 'bert.encoder.layer.10.attention.self.value', 'bert.encoder.layer.10.attention.self.dropout', 'bert.encoder.layer.10.attention.output', 'bert.encoder.layer.10.attention.output.dense', 'bert.encoder.layer.10.attention.output.LayerNorm', 'bert.encoder.layer.10.attention.output.dropout', 'bert.encoder.layer.10.intermediate', 'bert.encoder.layer.10.intermediate.dense', 'bert.encoder.layer.10.intermediate.intermediate_act_fn', 'bert.encoder.layer.10.output', 'bert.encoder.layer.10.output.dense', 'bert.encoder.layer.10.output.LayerNorm', 'bert.encoder.layer.10.output.dropout', 'bert.encoder.layer.11', 'bert.encoder.layer.11.attention', 'bert.encoder.layer.11.attention.self', 'bert.encoder.layer.11.attention.self.query', 'bert.encoder.layer.11.attention.self.key', 'bert.encoder.layer.11.attention.self.value', 'bert.encoder.layer.11.attention.self.dropout', 'bert.encoder.layer.11.attention.output', 'bert.encoder.layer.11.attention.output.dense', 'bert.encoder.layer.11.attention.output.LayerNorm', 'bert.encoder.layer.11.attention.output.dropout', 'bert.encoder.layer.11.intermediate', 'bert.encoder.layer.11.intermediate.dense', 'bert.encoder.layer.11.intermediate.intermediate_act_fn', 'bert.encoder.layer.11.output', 'bert.encoder.layer.11.output.dense', 'bert.encoder.layer.11.output.LayerNorm', 'bert.encoder.layer.11.output.dropout', 'bert.pooler', 'bert.pooler.dense', 'bert.pooler.activation', 'dropout', 'classifier']\n",
            "\n",
            "Trainable parameters for lora:\n",
            "trainable params: 1,181,186 || all params: 168,539,140 || trainable%: 0.7008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-9dcc6efe299b>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241108_071243-6o2k15fc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/pike/huggingface/runs/6o2k15fc' target=\"_blank\">/content/sample_data/best_models//MBERT_uncased_CrossEntropyLoss_lora</a></strong> to <a href='https://wandb.ai/pike/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/pike/huggingface' target=\"_blank\">https://wandb.ai/pike/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/pike/huggingface/runs/6o2k15fc' target=\"_blank\">https://wandb.ai/pike/huggingface/runs/6o2k15fc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/93 00:04 < 00:10, 6.02 it/s, Epoch 0.99/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='99' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 99/125 00:02 < 00:00, 33.06 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-127414b91707>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# # Run PEFT fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting PEFT Fine-Tuning with LoRA on all models...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mpeft_fine_tune_all_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# # Run inference and save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-58422999d686>\u001b[0m in \u001b[0;36mpeft_fine_tune_all_models\u001b[0;34m(model_checkpoints, dataset, loss_functions, peft_methods)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                         \u001b[0;31m# Save the model and adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2123\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2124\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2572\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2573\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   3002\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3004\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3006\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3974\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3975\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   3976\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3977\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4168\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4169\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4170\u001b[0m             \u001b[0mmain_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"main_input_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4171\u001b[0m             inputs_decode = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloss_without_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4384\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4385\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4386\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-9dcc6efe299b>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1512\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1515\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1668\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1669\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 515\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_dora\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_adapter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlora_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add any additional parameters to the configurations?\n",
        "# Modify the configurations for better performance?\n",
        "# Add more PEFT methods?\n",
        "# Make any other changes?"
      ],
      "metadata": {
        "id": "lec6PHkIkXdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}